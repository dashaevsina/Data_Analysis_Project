---
title: "Data Analysis Project code"
subtitle: "UN data clustering analysis"
output: html_notebook 
---

# Libraries
Load necessary libraries.

```{r}
library(pdftools)
library(tidyverse)

library(tm)

library(ggplot2)
library(lsa)

library(wordcloud)

library(magrittr)
library(dplyr)
library(ggpubr)

library(proxy)

library(cluster)
library(dendextend)

library(tidytext)
library(topicmodels)
```


# Create a vector
Create a vector data structure in R of the UN PDF file names using the `list.files` function. 
```{r}
all_files <- list.files(pattern = "pdf$")
all_files 

typeof(all_files)
```

In the `pdftools` library, the function pdf_text is used to extract text. Using the `lapply` function, the pdf_text function can be applied to each element in the “all_files” vector to then create an object called “UN_files”.
```{r}
UN_files <- lapply(all_files, pdf_text)
```

This creates a list object with a specific number of elements, one for each document. The `length` function verifies it contains xyz elements:
```{r}
length(UN_files)
```

Thus, each element is a vector that contains the text of the PDF file. The length of each vector corresponds to the number of pages in the PDF file. For example, the first vector has length xyz because the first PDF file has xyz pages. The `length` function can be applied to each element to see this:
```{r}
lapply(UN_files, length) 
```



# Textmining

Load the `tm package` and then create a corpus, which is essentially a database for the UN texts. 
```{r}
UN_corpus <- Corpus(URISource(all_files),
               readerControl = list(reader = readPDF))
```

## Term-document matrix
Having created a corpus called `UN_corpus`, a term-document matrix (TDM) can be generated, which stores counts of terms for each document. First, the punctuation marks have to be removed. 
```{r}
UN_corpus <- tm_map(UN_corpus, removePunctuation, ucp = TRUE)
```

The `tm package` includes a function to create a TDM called `TermDocumentMatrix`. NB: no stemming is applied.
```{r}
UN_corpus.tdm <- TermDocumentMatrix(UN_corpus, 
                                   control = 
                                     list(stopwords = TRUE,
                                          tolower = TRUE,
                                          stemming = TRUE,
                                          removeNumbers = TRUE,
                                          bounds = list(global = c(3, Inf))))
```

Inspect the UN_corpus term-document matrix:
```{r}
inspect(UN_corpus.tdm[1:10,]) 
```

### Summary statistics
There are a few functions for summary statistics in the `tm package`. For instance, the `findFreqTerms` function can find frequently occurring terms. Words that occur at least 100 times are shown here:
```{r}
findFreqTerms(UN_corpus.tdm, lowfreq = 100, highfreq = Inf)
```

The counts of these words can be shown in each of the UN report PDF documents:
```{r}
frequent_terms <- findFreqTerms(UN_corpus.tdm, lowfreq = 100, highfreq = Inf)
as.matrix(UN_corpus.tdm[frequent_terms,]) 
```

The total counts for the most frequent words can be saved into a matrix. The  sum function is subsequently applied across the rows:

```{r}
frequent_terms.tdm <- as.matrix(UN_corpus.tdm[frequent_terms,])
sort(apply(frequent_terms.tdm, 1, sum), decreasing = TRUE)
```

# Document-term matrix
NB: In a document-term matrix, rows represent documents in the collection and columns represent terms, whereas the term-document matrix is the transpose of DTM.
Create document-term matrix:
```{r}

UN_corpus <- tm_map(UN_corpus, removePunctuation, ucp = TRUE)

dtm = DocumentTermMatrix(UN_corpus,
                         control = list(
                                        stopwords = TRUE,
                                        tolower = TRUE,
                                        removeNumbers = TRUE,
                                        stemming = TRUE,
                                        bounds = list(global = c(3, Inf))))

dtm <- dtm[, names(head(sort(colSums(as.matrix(dtm))), 400))]
dtm <- dtm[, names(sort(colSums(as.matrix(dtm))))]

inspect(dtm)
```


## TF-IDF
Term frequency-inverse document frequency (TF-IDF):
```{r}
UN_dtm <- weightTfIdf(dtm, normalize = TRUE)
dtm.matrix = as.matrix(UN_dtm)

inspect(UN_dtm)
```



# Calculating Distance
The clustering algorithm uses distances to cluster documents.

## Eucledian distance
Use the `dist` function (for matrix distance/ similarity computation) to calculate the euclidean distance between the documents.
```{r}
UN_dtm <- as.matrix(UN_dtm)
distMatrix_Eucledian <- dist(UN_dtm, method="euclidean")
print(distMatrix_Eucledian)

# NB: For continuous numerical values, use Eucledian distance
```

## Cosine distance
Calculate cosine distance:
```{r}
UN_tdm <- as.matrix(UN_corpus.tdm)
cosine_dist_mat <- as.matrix(dist(t(UN_tdm), method = "cosine"))

cosine_dist_mat
```

The diagonal of the cosine distance matrix `cosine_dist_mat` should be removed, since the distance between a document and itself is neither informative, nor necessary.
```{r}
diag(cosine_dist_mat) <- NA
cosine_dist <- apply(cosine_dist_mat, 2, mean, na.rm=TRUE)

cosine_dist
```

# Clustering
The R algorithm `hclust` will be used for agglomerative (bottom-up) hierarchical clustering based on Eucledian distance. Hierarchical clustering is computationally inexpensive, as opposed to k-means clustering.
Ward's method is used as the merge rule. Other methods, such as `ward.D2`, "single", "complete", "average", "mcquitty", "median", or "centroid" can likewise be used.

The tree can be cut into clusters. These can be determined either arbitrarily  or based on one of three methods (the Elbow method, Average silhouette method or Gap statistic method). 
```{r}
groups_hierarchical <- hclust(distMatrix_Eucledian, method="ward.D")
clustering <- cutree(groups_hierarchical, 10)
plot(groups_hierarchical, 
     main="Hierarchical clustering of UN data",
       cex=0.9, hang=-1)
rect.hclust(groups_hierarchical, 10, border="mediumseagreen")
```

## Hierarchical clustering: "average" linkage method
In hierarchical agglomerative cluster analysis (HAC, for short), there are numerous linkage methods that can be used to generate the different clusters. Another option would be `average`, as shown below.
```{r}
library(dendextend) # an R package for creating and comparing visually appealing tree diagrams

hclust_average <- hclust(distMatrix_Eucledian, method = "average")
plot(hclust_average)
rect.hclust(hclust_average , k = 10, border = 2:6)
abline(h = 10, col = 'red')
```
The above dendrogram is produced, whereby each data point eventually merges into a single cluster with the height (or distance) indicated on the y-axis.

### Visualising the tree
This uses the `dendextend` library.
```{r}
library(dendextend)
average_dend_obj <- as.dendrogram(hclust_average)
average_column_dend <- color_branches(average_dend_obj, k = 10, h = 10)
plot(average_column_dend)
```
## "Centroid" linkage method
In hierarchical agglomerative cluster analysis (HAC, for short), there are numerous linkage methods that can be used to generate the different clusters. Another option would be `centroid`, as shown below.
```{r}
hclust_centroid <- hclust(distMatrix_Eucledian, method = "centroid")
centroid_dend_obj <- as.dendrogram(hclust_centroid)
centroid_column_dend <- color_branches(centroid_dend_obj, k = 12, h = 12)
plot(centroid_column_dend)
```
## "Mcquitty" linkage method
In hierarchical agglomerative cluster analysis (HAC, for short), there are numerous linkage methods that can be used to generate the different clusters. Another option would be `mcquitty`, as shown below.
```{r}
hclust_mcquitty <- hclust(distMatrix_Eucledian, method = "mcquitty")
mcquitty_dend_obj <- as.dendrogram(hclust_mcquitty)
mcquitty_column_dend <- color_branches(mcquitty_dend_obj, k = 15, h = 15)
plot(mcquitty_column_dend)
```

## Divisive hierarchical clustering
Divisive hierarchical clustering is the inverse of hierarchical clustering since it starts off with one cluster containing the whole dataset. Subsequently, the observation with the highest average dissimilarity (in other words, the one farthest from the cluster according to a specific metric) is reassigned to its own cluster.
It uses the `cluster` package and the `diana` (DIvisive ANAlysis) function.
```{r}
divisive_hclust <- diana(distMatrix_Eucledian, stand=TRUE)

divisive_dend_obj <- as.dendrogram(divisive_hclust)
divisive_column_dend <- color_branches(divisive_dend_obj, k = 15, h = 15)
plot(divisive_column_dend)
```

# LDA: Latent Dirichlet allocation
Latent Dirichlet allocation is a topic modelling algorithm that is based on two guiding principles: 
+ Every document is a mixture of topics;
+ Every topic is a mixture of words.
The package `topicmodels` is used for the LDA method.
```{r}
lda_text <- LDA(dtm, k=2, method = "VEM", control = NULL)
lda_text
```

The 10 most common words in each group topic can be visualising using the `tidytext` Text Mining package. Thus, the "beta" method is used in order to extract the per-topic-per-word probabilities.
```{r}
lda_topics <- tidy(lda_text, matrix = "beta")
lda_topics
```

The above tibble data frame can then be plotted.
```{r}
lda_top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n=10) %>%
  ungroup() %>%
  arrange(topic, -beta)

lda_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  labs(title="Top 10 most common terms per topic",
        x ="Term", y = "LDA (beta)") +
  theme(plot.title=element_text(color="coral4", size=16, face="bold.italic"),
        axis.title.x = element_text(color="coral4", size=14, face="bold"),
        axis.title.y = element_text(color="coral4", size=14, face="bold")) +
  scale_fill_manual(values = c("lightgoldenrod4", "salmon")) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~topic, scales="free_x") +
  coord_flip() 
```

## LDA Log ratio
The greatest difference in beta between the two topics can likewise be considered with the help of a log ratio.
This analysis relies on the `tidyr` package.
```{r}
lda_logRatio <- lda_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 0.001 | topic2 > 0.001) %>% # filter for common words that have a beta greater than 1/1000 in at least 1 topic
  mutate(log_ratio = log2(topic2 / topic1))

lda_logRatio
```
The above tibble data frame can be plotted as follows:
```{r}
lda_logRatio %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(15, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(title = "Terms with greatest difference between the 2 topics",
       y= "Log ratio of beta in topic 2 vs topic 1") +
  theme(plot.title=element_text(color="navy", size=20, face="bold.italic"),
        axis.title.x = element_text(color="navy", size=14, face="bold"),
        axis.title.y = element_text(color="navy", size=14, face="bold")) +
  scale_fill_manual(values = c("slateblue4", "gold2")) +
  coord_flip()
```

## LDA Document-topic probabilities
Latent Dirichlet Allocation can also model each UN General Assembly report (from 1991 to 2020) as a mixture of topics. In this case, the "gamma" method is used in order to extract the per-document-per-topic probabilities, in other words, it indicates the proportion of the document that consists of words from the assigned topic
```{r}
lda_documents <- tidy(lda_text, matrix ="gamma")
lda_documents
```

Each of these values is an estimated proportion of words from the UN document that are generated from either topic. For instance, the model estimates that approximately 48% of the words in the UNGA report from 1991 were generated from topic 2.
The tibble below demonstrates the top 5 UNGA reports per topic.
```{r}
top_n(lda_documents, 10)
```

A boxplot can illustrate how well the unsupervised learning algorithm distinguishes between the different topics for each of the UNGA reports.
```{r}
lda_documents %>%
  mutate(title=reorder(document, gamma*topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot(fill="gold", color="deeppink1") +
  facet_wrap(~document)
```


# Multidimensional scaling (MDS)
In data analysis, multidimensional scaling is a visual representation of information contained in a distance matrix that reflects the distances or dissimilarities between sets of objects.
```{r}
# Compute MDS
mds <- UN_tdm %>%
  dist() %>%          
  cmdscale() %>%
  as_tibble()
colnames(mds) <- c("Coordinate_1", "Coordinate_2") # unique column names!
```

An MDS plot arranges the points on the graph so that the distances among each pair of points correlates as accurately as possible to the dissimilarity between those two samples.
```{r}
# Plot MDS
ggscatter(mds, x = "Coordinate_1", y = "Coordinate_2", 
          label = rownames(UN_corpus.tdm),
          size = 1,
          repel = TRUE)
```


